#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
í”„ë¡œê·¸ë˜ë¨¸ìŠ¤ í”„ë¡œí•„ ìŠ¤í¬ë˜í¼
GitHub í”„ë¡œí•„ìš© í”„ë¡œê·¸ë˜ë¨¸ìŠ¤ í†µê³„ë¥¼ ê°€ì ¸ì˜¤ëŠ” ìŠ¤í¬ë¦½íŠ¸
"""

import requests
from bs4 import BeautifulSoup
import json
import time
import os
from datetime import datetime

class ProgrammersScraper:
    def __init__(self, username):
        self.username = username
        self.base_url = "https://programmers.co.kr"
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        })
    
    def get_user_profile(self):
        """í”„ë¡œê·¸ë˜ë¨¸ìŠ¤ ì‚¬ìš©ì í”„ë¡œí•„ ì •ë³´ ê°€ì ¸ì˜¤ê¸°"""
        try:
            # í”„ë¡œê·¸ë˜ë¨¸ìŠ¤ ì‚¬ìš©ì í”„ë¡œí•„ URL (ì‹¤ì œ ì‚¬ìš©ìëª…ìœ¼ë¡œ ë³€ê²½ í•„ìš”)
            profile_url = f"{self.base_url}/users/{self.username}"
            response = self.session.get(profile_url)
            
            if response.status_code == 200:
                soup = BeautifulSoup(response.content, 'html.parser')
                return self._parse_profile(soup)
            else:
                print(f"í”„ë¡œí•„ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {response.status_code}")
                return None
                
        except Exception as e:
            print(f"í”„ë¡œí•„ ê°€ì ¸ì˜¤ê¸° ì˜¤ë¥˜: {e}")
            return None
    
    def _parse_profile(self, soup):
        """HTMLì—ì„œ í”„ë¡œí•„ ì •ë³´ íŒŒì‹±"""
        profile_data = {
            'username': self.username,
            'level': 'N/A',
            'solved_problems': 0,
            'rank': 'N/A',
            'last_updated': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        }
        
        try:
            # ë ˆë²¨ ì •ë³´ ì°¾ê¸°
            level_elements = soup.find_all('div', class_='level')
            if level_elements:
                profile_data['level'] = level_elements[0].get_text().strip()
            
            # í•´ê²°í•œ ë¬¸ì œ ìˆ˜ ì°¾ê¸°
            solved_elements = soup.find_all('span', class_='solved-count')
            if solved_elements:
                profile_data['solved_problems'] = int(solved_elements[0].get_text().strip())
            
            # ìˆœìœ„ ì •ë³´ ì°¾ê¸° (ìˆëŠ” ê²½ìš°)
            rank_elements = soup.find_all('div', class_='rank')
            if rank_elements:
                profile_data['rank'] = rank_elements[0].get_text().strip()
                
        except Exception as e:
            print(f"í”„ë¡œí•„ íŒŒì‹± ì˜¤ë¥˜: {e}")
        
        return profile_data
    
    def get_recent_solutions(self, limit=5):
        """ìµœê·¼ í•´ê²°í•œ ë¬¸ì œë“¤ ê°€ì ¸ì˜¤ê¸°"""
        try:
            solutions_url = f"{self.base_url}/users/{self.username}/solutions"
            response = self.session.get(solutions_url)
            
            if response.status_code == 200:
                soup = BeautifulSoup(response.content, 'html.parser')
                return self._parse_solutions(soup, limit)
            else:
                return []
                
        except Exception as e:
            print(f"ìµœê·¼ í•´ê²° ë¬¸ì œ ê°€ì ¸ì˜¤ê¸° ì˜¤ë¥˜: {e}")
            return []
    
    def _parse_solutions(self, soup, limit):
        """í•´ê²°í•œ ë¬¸ì œ ëª©ë¡ íŒŒì‹±"""
        solutions = []
        try:
            solution_elements = soup.find_all('div', class_='solution-item')[:limit]
            
            for element in solution_elements:
                solution = {
                    'title': element.find('h3').get_text().strip() if element.find('h3') else 'N/A',
                    'language': element.find('span', class_='language').get_text().strip() if element.find('span', class_='language') else 'N/A',
                    'date': element.find('span', class_='date').get_text().strip() if element.find('span', class_='date') else 'N/A'
                }
                solutions.append(solution)
                
        except Exception as e:
            print(f"í•´ê²° ë¬¸ì œ íŒŒì‹± ì˜¤ë¥˜: {e}")
        
        return solutions

def generate_programmers_stats_html(profile_data, solutions):
    """í”„ë¡œê·¸ë˜ë¨¸ìŠ¤ í†µê³„ë¥¼ HTMLë¡œ ìƒì„±"""
    html = f"""
    <div align="center">
        <h3>ğŸ† í”„ë¡œê·¸ë˜ë¨¸ìŠ¤ í†µê³„</h3>
        <table>
            <tr>
                <td><strong>ë ˆë²¨:</strong></td>
                <td>{profile_data['level']}</td>
            </tr>
            <tr>
                <td><strong>í•´ê²°í•œ ë¬¸ì œ:</strong></td>
                <td>{profile_data['solved_problems']}ë¬¸ì œ</td>
            </tr>
            <tr>
                <td><strong>ìˆœìœ„:</strong></td>
                <td>{profile_data['rank']}</td>
            </tr>
        </table>
        
        <h4>ğŸ“ ìµœê·¼ í•´ê²°í•œ ë¬¸ì œ</h4>
        <ul>
    """
    
    for solution in solutions:
        html += f"""
            <li>
                <strong>{solution['title']}</strong> 
                ({solution['language']}) - {solution['date']}
            </li>
        """
    
    html += """
        </ul>
        <p><em>ë§ˆì§€ë§‰ ì—…ë°ì´íŠ¸: {}</em></p>
    </div>
    """.format(profile_data['last_updated'])
    
    return html

def update_readme_with_programmers_data():
    """README.md íŒŒì¼ì„ í”„ë¡œê·¸ë˜ë¨¸ìŠ¤ ë°ì´í„°ë¡œ ì—…ë°ì´íŠ¸"""
    # ì‹¤ì œ í”„ë¡œê·¸ë˜ë¨¸ìŠ¤ ì‚¬ìš©ìëª…ìœ¼ë¡œ ë³€ê²½í•˜ì„¸ìš”
    scraper = ProgrammersScraper('your_programmers_username')
    
    # í”„ë¡œí•„ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
    profile_data = scraper.get_user_profile()
    if not profile_data:
        profile_data = {
            'username': 'your_programmers_username',
            'level': 'N/A',
            'solved_problems': 0,
            'rank': 'N/A',
            'last_updated': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        }
    
    # ìµœê·¼ í•´ê²°í•œ ë¬¸ì œ ê°€ì ¸ì˜¤ê¸°
    solutions = scraper.get_recent_solutions(5)
    
    # HTML ìƒì„±
    programmers_html = generate_programmers_stats_html(profile_data, solutions)
    
    # README.md íŒŒì¼ ì½ê¸°
    with open('README.md', 'r', encoding='utf-8') as f:
        readme_content = f.read()
    
    # í”„ë¡œê·¸ë˜ë¨¸ìŠ¤ í†µê³„ ë¶€ë¶„ êµì²´
    start_marker = '<!-- í”„ë¡œê·¸ë˜ë¨¸ìŠ¤ í†µê³„ëŠ” APIë‚˜ ì›¹ ìŠ¤í¬ë˜í•‘ì„ í†µí•´ ë™ì ìœ¼ë¡œ ì—…ë°ì´íŠ¸ë©ë‹ˆë‹¤ -->'
    end_marker = '## ğŸ“ˆ ìµœê·¼ í™œë™'
    
    start_idx = readme_content.find(start_marker)
    end_idx = readme_content.find(end_marker)
    
    if start_idx != -1 and end_idx != -1:
        new_content = (
            readme_content[:start_idx] +
            start_marker + '\n' +
            programmers_html + '\n' +
            readme_content[end_idx:]
        )
        
        # README.md íŒŒì¼ ì—…ë°ì´íŠ¸
        with open('README.md', 'w', encoding='utf-8') as f:
            f.write(new_content)
        
        print("README.md íŒŒì¼ì´ ì„±ê³µì ìœ¼ë¡œ ì—…ë°ì´íŠ¸ë˜ì—ˆìŠµë‹ˆë‹¤!")
    else:
        print("README.md íŒŒì¼ì—ì„œ ë§ˆì»¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    print("í”„ë¡œê·¸ë˜ë¨¸ìŠ¤ ìŠ¤í¬ë˜í¼ ì‹œì‘...")
    update_readme_with_programmers_data()
    print("ì™„ë£Œ!") 